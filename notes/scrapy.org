* configuration settings
** /etc/scrapy.cfg > 个人设置 ~/.config/scrapy.cfg > 项目配置scrapy.cfg
** 环境变量设置

* 命令行
** scrapy 全局命令
** 有些命令只能在项目内执行
*** crawl 执行爬命令
*** shell 调试比较常用

* spiders
处理一个被爬下来的网站，基本是链接处理，结构化数据。
用户为网站单独定制的行为
** 流程
   1. 从一堆urls里开始发送请求
   2. 在回调函数里处理页面，比如用selector解析page
   3. 回调函数通常返回items或者接着返回新的response请求
   4. 这些items通过pipeline处理存储，输出等
** 常用spider类
   1. crawlSpider. 通过rule正则匹配来爬虫，比较常用可扩展的类（parse方法有用）
   2. xmlfeedSpider. 通过迭代各个节点来分析xml源， 类似的csvfeedSpider
   3. sitemapSpider. 通过robots.txt机器人协议来爬网站
   
* Selectors ::解析爬到的response
** 工作方式
   基于文本或者response构造
   利用xpath协议或者css协议查找html页面的节点，lxml实现方式
   结合正则表达式可以达到很强大的功能了，注意用extract方法获取原文内容

* Items
  scrapy spider可以返回python字典，但是现在提供一种更上层的封装，Item类
** Field类其实才是原生的python dict
** Item 继承基类mapping 和 trackref
   ＋trackref 纪录new出来的对象的信息，便于debug
   ＋mapping 管理key－value结构

* Item Loader
  虽然可以直接操作item类，因为就是dict结构，但是这是一个封装的控制器
  add_xpath, add_css, add_value, 填充数据，最后load_item执行合并
  override之后可以定义加入时的行为

* Item Pipeline
** 典型应用
   ＋ 清理HTML数据
   ＋ 验证爬取的数据(检查item包含某些字段)	
   ＋ 查重(并丢弃)
   ＋ 将爬取结果保存到数据库中
** 用法
   override function process_item()
   
* Feed exports
  通用的几种存储方式

* 请求和回应
** TODO Request
   生命周期从spider创建，在downloader中结束，downloader中执行网络请求并返回Response
   查看具体类实现
   
* Link Extractors
  用于从网页中抽取会被follow的链接对象
  
* tor :ip pool:
  防止ip被ban，利用tor在底层建立链路（网桥）代理，实现混淆ip的作用
  socks5代理，默认端口9050
** meek
   因为tor服务的很多ip地址直接被gfw屏蔽，所以利用meek混淆，寻找云端的ip池，加密等技术绕过屏蔽
   meek用golang编译，在torrc里配置一个meek-client
** polipo
   一个更加底层的轻量级代理，将tor搭在其上运行。http默认端口8123
